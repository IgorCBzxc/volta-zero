{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da87421f-2533-4f0f-9df1-61d76430e0bc",
   "metadata": {},
   "source": [
    "# Tarefa 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8c515-70bd-4e3d-a35e-267fc6101275",
   "metadata": {},
   "source": [
    "**1.** Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b6575-ec87-489c-81f1-b49a29cbba70",
   "metadata": {},
   "source": [
    "| Diferenças | AdaBoost | GBM |\n",
    "| ---------- | -------- | --- |\n",
    "| 1 | Utiliza um conjunto de classificadores fracos (*Stumps*), também conhecidos como *weak/base learners*. | Utiliza árvores mais complexas do que o *AdaBoost*. |\n",
    "| 2 | As árvores de decisão são extremamente simples com apenas 1 de profundidade e 2 folhas. | As árvores complexas podem ser podadas ou não. |\n",
    "| 3 | O primeiro passo do modelo é a seleção de um *Stump* com o melhor desempenho. | O primeiro passo do modelo é o cálculo da média do Y (resposta/*target*). |\n",
    "| 4 | Cada resposta tem um peso diferente para a agregação final da predição. | A predição das respostas das árvores é calculada através de um multiplicador em comum chamado *learning_rate* (eta). |\n",
    "| 5 | A predição final é definida com uma votação majoritária ponderada das respostas de acordo com a performance de cada *Stump*. | A predição é baseada no ajuste do modelo através da redução dos erros residuais. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6d6ef-fc2c-4a3b-af12-84f47a045206",
   "metadata": {},
   "source": [
    "**2.** Acesse o link [Scikit-learn – GBM](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ae2ef-1da6-4003-88b2-07493906dc17",
   "metadata": {},
   "source": [
    "> [1.11. Ensemble methods — scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/ensemble.html#)\n",
    ">> [1.11.4.1. Classification](https://scikit-learn.org/stable/modules/ensemble.html#classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c18906a-757b-4c3b-8c96-d810bcafbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41a352d-e340-44bd-b6bf-431f9ac42aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, \n",
    "                                 learning_rate=1.0, \n",
    "                                 max_depth=1, \n",
    "                                 random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b0c1c-4407-4b0c-b4c9-e6fb8efd7bf2",
   "metadata": {},
   "source": [
    "> [1.11. Ensemble methods — scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/ensemble.html#)\n",
    ">> [1.11.4.2. Regression](https://scikit-learn.org/stable/modules/ensemble.html#regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39199baf-62e1-493b-a692-0fb21529054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a068a940-b305-4c6a-bccb-cef99e59d0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, \n",
    "                                learning_rate=0.1, \n",
    "                                max_depth=1, \n",
    "                                random_state=0, \n",
    "                                loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875633aa-d96e-460b-8487-a6743ef61926",
   "metadata": {},
   "source": [
    "**3.** Cite 5 hiperparâmetros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86622a0-b73c-4c06-84dd-76f20ddf2fe9",
   "metadata": {},
   "source": [
    "> **1. `n_estimators`**: Este importante parâmetro controla o número de árvores no modelo. Um valor maior pode aumentar tanto o tempo de treinamento quanto a capacidade de aprendizado do modelo.\n",
    ">\n",
    "> **2. `learning_rate`**: É um importante hiperparâmetro no intervalo de 0.0 a 1.0 que controla a taxa de aprendizado do modelo. Interage fortemente com o número de estimadores (*n_estimators*). O valor ideal pode variar dependendo do problema e dos dados.\n",
    ">\n",
    "> **3. `max_depth`**: Controla a profundidade máxima de cada árvore no modelo, especificando o número máximo de nós. Uma profundidade maior permite que as árvores sejam mais complexas e tenham maior capacidade de aprendizado, mas também aumenta o risco de *overfitting*.\n",
    ">\n",
    "> **4. `max_features`**: Controla o número de variáveis consideradas ao fazer uma divisão em cada nó das árvores. Quanto menor o valor, menor é o risco de *overfitting*.\n",
    ">\n",
    "> **5. `warm_start`**: Permite adicionar mais estimadores a um modelo já ajustado. Pode ser útil quando é necessário ajustar o modelo com mais árvores sem perder o progresso anterior.\n",
    ">> Exemplo:\n",
    ">>```python\n",
    ">>>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    ">>>>> _ = est.fit(X_train, y_train)  # fit additional 100 trees to est\n",
    ">>>>> mean_squared_error(y_test, est.predict(X_test))\n",
    ">>3.84...\n",
    ">>```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd77068-5da9-48bf-b7b7-c88ff2978b5e",
   "metadata": {},
   "source": [
    "**5.** Acessando o artigo do Jerome Friedman ([Stochastic](https://jerryfriedman.su.domains/ftp/stobst.pdf)) e pensando no nome dado ao **Stochastic GBM**, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3fde36-2bb7-46f9-ba26-55627bea0d8a",
   "metadata": {},
   "source": [
    "> Pensando no nome dado ao *Stochastic GBM* e com referência à teoria probabilística, podemos descrevê-lo como um algoritmo que incorpora variáveis aleatórias. O *Stochastic GBM* é uma combinação dos métodos de *Gradient Boosting* e *Bootstrap Aggregating*, sendo considerado um híbrido das técnicas *Bagging* e *Boosting*. Em cada iteração, o classificador base é treinado em um subconjunto aleatório e não repetitivo dos dados de treinamento, utilizando em média metade da amostra. Essa aleatoriedade na amostragem do conjunto de dados em cada iteração de treino melhora significativamente a precisão do *Gradient Boosting* e torna o modelo mais robusto em comparação com o GBM tradicional. Isso ocorre porque a aleatoriedade ajuda a evitar o *overfitting* e promove uma melhor generalização do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4448a-5921-4d47-84af-b29a7f008175",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc89696-b7e1-4be3-b493-c6b863fa337f",
   "metadata": {},
   "source": [
    "# Tarefa 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05220745-5553-46bf-9721-3e16cccd3285",
   "metadata": {},
   "source": [
    "**1.** Cite 5 diferenças entre o Random Forest e o AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db295d28-729f-4b9f-943b-bc31ccb47ce2",
   "metadata": {},
   "source": [
    "| Diferenças | Random Forest | AdaBoost|\n",
    "| --- | --- | --- |\n",
    "| 1 | Utiliza uma \"floresta\" de árvores com a possibilidade de serem completas ou podadas. | Utiliza por padrão, uma floresta de *Stumps* (classificadores fracos), ou seja, árvores de decisão extremamente simples com apenas 1 de profundidade e 2 folhas. |\n",
    "| 2 | Utiliza árvores independentes que podem rodar paralelamente com os conjuntos de dados individualmente criados por amostragem aleatória com repetição (*Bootstrap*). | As árvores (*Stumps*) são influenciadas entre si sequencialmente e os conjuntos de dados sequenciais são criados também por amostragem aleatória com repetição, porém de forma  ponderada com base no \"peso\" de cada linha através do cálculo de performance do melhor *Stump*, onde exemplos classificados incorretamente têm maior probabilidade de serem selecionados. |\n",
    "| 3 | As respostas tem o mesmo peso (importância) independente da qualidade de classificação da árvore. | As respostas tem pesos diferentes, influenciando consequentemente nas relevâncias de cada resposta. |\n",
    "| 4 | Utiliza uma quantidade definida de variáveis aleatórias (*feature selection*) em cada árvore. | Utiliza somente uma variável explicativa para cada *Stump*. |\n",
    "| 5 | Utiliza o procedimento de votação majoritária simples como previsão. | Utiliza como predição final uma votação majoritária ponderada das respostas de acordo com a performance de cada *Stump*. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89239d72-96bc-47e4-b991-b1e709be253c",
   "metadata": {},
   "source": [
    "**2.** Acesse o link [Scikit-learn – adaboost](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a1819c-32c7-4923-a726-316782116e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets        import load_iris\n",
    "from sklearn.ensemble        import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0424eee9-4d7c-4b5d-a49e-db5a1aba574f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y   = load_iris(return_X_y=True)\n",
    "\n",
    "clf    = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "scores = cross_val_score(estimator=clf, \n",
    "                         X=X, \n",
    "                         y=y, \n",
    "                         cv=5)\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49e8a9-17c5-4a26-8acc-c73363ef763e",
   "metadata": {},
   "source": [
    "**3.** Cite 5 Hiperparâmetros importantes no AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1efa6a0-234a-4c6e-a673-320350573444",
   "metadata": {},
   "source": [
    "> **1. `estimator`**: Classificador base para criar os *weak/base learners (Stumps)*. Por padrão o estimador base é o algoritmo de árvore de decisão `DecisionTreeClassifier(max_depth=1)`.\n",
    ">> Sucessor do hiperparâmetro descontinuado `base_estimator`. \n",
    ">\n",
    "> **2. `n_estimators`**: Número máximo de iterações para finalizar o processo de aprendizagem. Pode ser interrompido antes deste limite caso seja considerado perfeito. Quanto maior o número de estimadores, maior será o tempo de treinamento.\n",
    ">> Quantidade de *Stumps*.\n",
    ">\n",
    "> **3. `learning_rate`**: Valor do peso aplicado a cada classificador nas iterações de reforço. A relevância dos *Stumps* pode aumentar conforme essa definição, podendo haver uma compensação entre este hiperparâmetro e o *n_estimator*.\n",
    ">> Taxa de aprendizado do *AdaBoost*.\n",
    ">\n",
    "> **4. `algorithm (AdaBoostClassifier)`**: Define o algoritmo utilizado para atualizar os pesos dos exemplos durante o treinamento. \n",
    ">> Pode ser `SAMME` (*Discrete AdaBoost*) ou `SAMME.R` (*Real AdaBoost*), onde `SAMME.R` é recomendado para melhor desempenho.\n",
    ">\n",
    "> **5. `loss (AdaBoostRegressor)`**: Assim como o hiperparâmetro *algorithm*, porém em problemas de regressão, este hiperparâmetro define a \"função de perda\" a ser usada ao atualizar os pesos após cada iteração de reforço.\n",
    ">> Pode ser 'linear', 'square' ou 'exponential', tendo 'linear' como padrão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921159d5-086f-44d0-a8dc-1f222c0e7d9e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
